{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import helpers as hp\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "import pickle\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Number of GPUs available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = 1\n",
    "load_mod = 0\n",
    "\n",
    "source_cov = hp.source_cov\n",
    "source_pne = hp.source_pne\n",
    "\n",
    "# set to image directory\n",
    "cov = glob.glob(source_cov+'*')\n",
    "pne = glob.glob(source_pne+'*')\n",
    "\n",
    "# parameters\n",
    "cov_train_num = hp.cov_train_num\n",
    "pne_train_num = hp.pne_train_num\n",
    "cov_avg_num = 5\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random subset of images for training\n",
    "if load_dir:\n",
    "\twith open('directory.pkl','rb') as f:\n",
    "\t\tcov_train, pne_train = pickle.load(f)\n",
    "\tcov_train = [source_cov+img.split(os.sep)[-1] for img in cov_train]\n",
    "\tpne_train = [source_pne+img.split(os.sep)[-1] for img in pne_train]\n",
    "else:\n",
    "\tcov_train = np.random.choice(cov,size=cov_train_num,replace=False)\n",
    "\tpne_train = np.random.choice(pne,size=pne_train_num,replace=False)\n",
    "\twith open('directory.pkl','wb') as f:\n",
    "\t\tpickle.dump([cov_train, pne_train],f)\n",
    "\n",
    "cov_test = list(set(cov)-set(cov_train))\n",
    "pne_test = list(set(pne)-set(pne_train))\n",
    "cov_avg = np.random.choice(cov_test,size=cov_avg_num,replace=False)\n",
    "cov_test = list(set(cov_test)-set(cov_avg))\n",
    "\n",
    "cov_test_num = len(cov_test)\n",
    "pne_test_num = len(pne_test)\n",
    "\n",
    "print('COVID training set size: '+str(cov_train_num))\n",
    "print('Pneumonia training set size: '+str(pne_train_num))\n",
    "print('Total training set size: '+str(cov_train_num+pne_train_num))\n",
    "print()\n",
    "print('COVID testing set size: '+str(cov_test_num))\n",
    "print('Pneumonia testing set size: '+str(pne_test_num))\n",
    "print('Total testing set size: '+str(cov_test_num+pne_test_num))\n",
    "print()\n",
    "print('Number of COVID pictures to average for classifier: '+str(cov_avg_num))\n",
    "print()\n",
    "print('Maximum training pairs size:')\n",
    "n = min([cov_train_num,pne_train_num])\n",
    "n1 = cov_train_num\n",
    "n2 = pne_train_num\n",
    "print('     With class balance: '+str(2*n**2-n))\n",
    "print('     Without class balance: '+str((n1*(n1-1))/2+(n2*(n2-1))/2+n1*n2))\n",
    "print('Maximum testing pairs size:')\n",
    "n = min([cov_test_num,pne_test_num])\n",
    "n1 = cov_test_num\n",
    "n2 = pne_test_num\n",
    "print('     With class balance: '+str(2*n**2-n))\n",
    "print('     Without class balance: '+str((n1*(n1-1))/2+(n2*(n2-1))/2+n1*n2))\n",
    "print('Maximum classification pairs size:')\n",
    "print('     With class balance: '+str(n*2))\n",
    "print('     Without class balance: '+str(n1+n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set following parameters to desired number of pairs according to output of above\n",
    "train_balance = 1;\n",
    "test_balance = 1;\n",
    "cl_balance = 0;\n",
    "num_tr_pairs = 4096;\n",
    "num_te_pairs = 1431;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "IMG_DIM = (IMG_WIDTH,IMG_HEIGHT)\n",
    "\n",
    "# load training images\n",
    "cov_train_imgs = [img_to_array(load_img(img,target_size=IMG_DIM,color_mode=\"rgb\")) for img in cov_train]\n",
    "pne_train_imgs = [img_to_array(load_img(img,target_size=IMG_DIM,color_mode=\"rgb\")) for img in pne_train]\n",
    "\n",
    "# create corresponding labels\n",
    "train_imgs = np.array(cov_train_imgs+pne_train_imgs)\n",
    "train_imgs_scaled = train_imgs.astype('float32')/255\n",
    "train_labels = np.array(cov_train_num*[1]+pne_train_num*[0])\n",
    "train_labels_enc = to_categorical(train_labels)\n",
    "\n",
    "# load test images and create corresponding labels\n",
    "cov_test_imgs = [img_to_array(load_img(img,target_size=IMG_DIM,color_mode=\"rgb\")) for img in cov_test]\n",
    "pne_test_imgs = [img_to_array(load_img(img,target_size=IMG_DIM,color_mode=\"rgb\")) for img in pne_test]\n",
    "test_imgs = np.array(cov_test_imgs+pne_test_imgs)\n",
    "test_imgs_scaled = test_imgs.astype('float32')/255\n",
    "test_labels = np.array(cov_test_num*[1]+pne_test_num*[0])\n",
    "test_labels_enc = to_categorical(test_labels)\n",
    "\n",
    "# create average covid image\n",
    "covavg_imgs = np.array([img_to_array(load_img(img,target_size=IMG_DIM,color_mode=\"rgb\")) for img in cov_avg])\n",
    "covavg_imgs_scaled = covavg_imgs.astype('float32')/255\n",
    "if len(covavg_imgs_scaled.shape)>3:\n",
    "    covavg_imgs_scaled = np.expand_dims(np.mean(covavg_imgs_scaled,axis=0),axis=0)\n",
    "plt.title('Average COVID image')\n",
    "plt.imshow(array_to_img(covavg_imgs_scaled[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create positive and negative pairs\n",
    "idx = [np.where(train_labels==i)[0] for i in range(num_classes)]\n",
    "tr_pairs, tr_y = hp.create_all_pairs(num_classes,train_imgs_scaled,idx,train_balance)\n",
    "\n",
    "idx = [np.where(test_labels==i)[0] for i in range(num_classes)]\n",
    "te_pairs, te_y = hp.create_all_pairs(num_classes,test_imgs_scaled,idx,test_balance)\n",
    "cl_pairs, cl_y = hp.covavg_pairs(num_classes,covavg_imgs_scaled,test_imgs_scaled,idx,cl_balance)\n",
    "\n",
    "# use random subset of all pairs (might need to adjust to ensure class balance)\n",
    "idx = random.sample(range(tr_pairs.shape[0]),num_tr_pairs)\n",
    "tr_pairs = np.array([tr_pairs[x] for x in idx])\n",
    "tr_y = np.array([tr_y[x] for x in idx])\n",
    "\n",
    "idx = random.sample(range(te_pairs.shape[0]),num_te_pairs)\n",
    "te_pairs = np.array([te_pairs[x] for x in idx])\n",
    "te_y = np.array([te_y[x] for x in idx])\n",
    "\n",
    "print('Balancing training pairs: '+str(train_balance==1))\n",
    "print('Balancing testing pairs: '+str(test_balance==1))\n",
    "print('Balancing classification pairs: '+str(cl_balance==1))\n",
    "print()\n",
    "print('Training pairs size: '+str(tr_pairs.shape[0]))\n",
    "print('Testing pairs size: '+str(te_pairs.shape[0]))\n",
    "print('Classification pairs size: '+str(cl_pairs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create siamese network with euclidean distance as final layer\n",
    "input_shape = (IMG_HEIGHT,IMG_WIDTH,train_imgs.shape[3])\n",
    "base_network = hp.vggnet_base(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(hp.euclidean_distance,output_shape=hp.eucl_dist_output_shape)([processed_a, processed_b])\n",
    "vgg_siamese = Model([input_a, input_b], distance)\n",
    "vgg_siamese.summary()\n",
    "for layer in vgg_siamese.layers[2].layers:\n",
    "    print(layer, layer.trainable)\n",
    "vgg_siamese.layers[2].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train or load model\n",
    "generator = 1\n",
    "batch_size = hp.batch_size\n",
    "adm = optimizers.Adam(lr=1e-4)\n",
    "if load_mod:\n",
    "\tvgg_siamese = load_model('vggtwin_tr11175_ep50.h5',compile=False)\n",
    "\tvgg_siamese.compile(loss=hp.contrastive_loss, optimizer=adm, metrics=[hp.accuracy])\n",
    "else:\n",
    "\tepochs = 20\n",
    "\tvgg_siamese.compile(loss=hp.contrastive_loss, optimizer=adm, metrics=[hp.accuracy])\n",
    "\tif generator:\n",
    "\t\tvgg_siamese.fit_generator(hp.trainGenerator2(tr_pairs[:, 0],tr_pairs[:, 1],tr_y),\n",
    "\t\tsteps_per_epoch=np.ceil(tr_pairs.shape[0]/batch_size),\n",
    "\t\tepochs=epochs,\n",
    "\t\tvalidation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),\n",
    "\t\tshuffle=True,\n",
    "\t\tverbose=1)\n",
    "\n",
    "\telse:\n",
    "\t\tvgg_siamese.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\tepochs=epochs,\n",
    "\t\t\t\tvalidation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y), # validate on small subset of testing dataset\n",
    "\t\t\t\tshuffle=True)\n",
    "\n",
    "\tvgg_siamese.save('vggtwin_tr'+str(tr_pairs.shape[0])+'_ep'+str(epochs)+'_g'+str(generator)+'.h5')\n",
    "    \n",
    "tr_y_dist = vgg_siamese.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "te_y_dist = vgg_siamese.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "cl_y_dist = vgg_siamese.predict([cl_pairs[:, 0], cl_pairs[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y_dist = vgg_siamese.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "te_y_dist = vgg_siamese.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "cl_y_dist = vgg_siamese.predict([cl_pairs[:, 0], cl_pairs[:, 1]])\n",
    "\n",
    "tr_acc = hp.compute_accuracy(tr_y, tr_y_dist)\n",
    "te_acc = hp.compute_accuracy(te_y, te_y_dist)\n",
    "cl_acc = hp.compute_accuracy(cl_y, cl_y_dist)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n",
    "print('* Accuracy on classification set: %0.2f%%' % (100 * cl_acc))\n",
    "print()\n",
    "\n",
    "threshold = hp.threshold\n",
    "cl_y_pred = hp.generate_label(cl_y_dist,threshold) # using half of margin for threshold\n",
    "tp,tn,fp,fn,sensitivity,specificity = hp.generate_metrics(cl_y,cl_y_pred)\n",
    "print('True positives: '+str(tp))\n",
    "print('True negatives: '+str(tn))\n",
    "print('False positives: '+str(fp))\n",
    "print('False negatives: '+str(fn))\n",
    "print('Sensitivity: '+str(sensitivity))\n",
    "print('Specificity: '+str(specificity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1100px",
    "right": "20px",
    "top": "93px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
